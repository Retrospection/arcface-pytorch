1921 train iters per epoch:
DataParallel(
  (module): ResNetFace(
    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (prelu): PReLU(num_parameters=1)
    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): IRBlock(
        (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=64, out_features=4, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=4, out_features=64, bias=True)
            (3): Sigmoid()
          )
        )
      )
      (1): IRBlock(
        (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=64, out_features=4, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=4, out_features=64, bias=True)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer2): Sequential(
      (0): IRBlock(
        (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=8, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
      )
      (1): IRBlock(
        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=8, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer3): Sequential(
      (0): IRBlock(
        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=256, out_features=16, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=16, out_features=256, bias=True)
            (3): Sigmoid()
          )
        )
      )
      (1): IRBlock(
        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=256, out_features=16, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=16, out_features=256, bias=True)
            (3): Sigmoid()
          )
        )
      )
    )
    (layer4): Sequential(
      (0): IRBlock(
        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=512, out_features=32, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=32, out_features=512, bias=True)
            (3): Sigmoid()
          )
        )
      )
      (1): IRBlock(
        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=512, out_features=32, bias=True)
            (1): PReLU(num_parameters=1)
            (2): Linear(in_features=32, out_features=512, bias=True)
            (3): Sigmoid()
          )
        )
      )
    )
    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout): Dropout(p=0.5)
    (fc5): Linear(in_features=32768, out_features=512, bias=True)
    (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
[[ 1.0481628  -0.27608973 -1.8984001  ... -0.35376966 -1.7710596
   0.88741803]
 [ 2.3049824  -0.6423985   1.5065969  ...  3.4702027  -0.04149984
   0.9766071 ]
 [ 0.7198591  -1.0766699   0.6519317  ...  0.63375425 -0.86241466
  -1.5950929 ]
 ...
 [-1.6555068   0.7039765   1.91001    ... -1.1204524  -2.942465
   0.7603084 ]
 [-1.0281028  -0.6624625   0.6821386  ...  1.4996682   0.62767494
   2.0517998 ]
 [ 2.0682838   2.269904    0.11937065 ... -0.04062725  0.40421426
  -2.1588202 ]]
[ 2409  8277  6684  6522  1058 10242    73  1927  1900  5820  5327  1810
  7868  8216  2810  1194  6100  2795 10440  7656  6193  8809  8254  8556
  8213   491  2119   927  9632  2659  8736  1762  6569  9414  1693  5864
  7420  3038 10340   131  4988  2087 10060  5328 10062  7087    48  8758
  8142  5440  3156  1384   292  1192  2503  3947  6122  7960  9692  8465
  4417  9108  3753  6699  8661  3808 10414  3910  4257   524  6888 10234
  8030  6496  9121  7865  3335  4163  4597    12  5232  5320 10300  6736
  4875  3182  4953  3956   967  7654  6711  6086   181  2552  8926  3213
  3071  8862  9152  4522  3896  1220  2566  1059  6191  7207  5765  5789
  3949  2054  7716   288  2529  1064  6600  8573  6722  9017  5055  3402
  7936 10222 10426  3759  9693   360  5262   890  5940  1964  9755  4308
  1503  9125  4840  5499  1376  3685   915  2044  6943  1919 10398  1030
  9982  3952  5921  1305  3810  2073  6901  6762  4042  7883  9469   731
  8540  3850  8015  4557  9502  8996  8407  4381  6240  4746   560   219
  9369    18  7890  5648  8757  8609  7901 10539  2990  1282  3240  9814
  5457 10089  7634  9344  8509  1895  8072  8656  7379  6599  5687  2838
 10002  3663  4273  2978  2921    80  3825  6351  8202  2470  1652 10189
  7785  6791  5869  8885  4885  2658  3798  5533  3277  6278  7010  8134
  1084  5526 10292  2989  4909  7108  9839  5398  9653  6541  3703  2280
  4057  2324  8378  5457  9648  7658    47  4214  9641   362 10489  2862
  7290  9657  8613  6429  8088  2002  6082  5044  9917  3299  4312  6856
  7022  5266  8572  3576]
Mon Dec 17 23:29:22 2018 || train epoch 0 || iter 0 || 7.556650547881415 iters/s || loss 24.529695510864258 || acc 0.0
[[ 1.082272    1.1375893  -2.4991934  ...  0.44331053  2.0117178
  -1.5448819 ]
 [ 1.8437161  -1.6266595  -2.9826388  ... -0.42688507  1.971909
   0.8453858 ]
 [ 1.797155    0.23375803 -2.18219    ...  2.5183403  -0.22855978
   1.2196684 ]
 ...
 [ 3.2632542   3.1023502  -1.242105   ...  0.06162571 -1.1560124
   1.869     ]
 [ 2.3099809   3.4671397  -1.1272881  ... -0.53552514  1.775777
  -0.0383538 ]
 [-0.9541716  -0.54373515 -1.6537696  ...  0.50440776  1.6176306
  -2.416506  ]]
[ 1164  1722  6942  3056  3755  7191  3056  3755  9148  1722  8294  1164
 10372  5208  3592  9609  6774  4823 10419  4621  6260  9718  4896  4182
  7191  9456  2580  9693  1659  3113  9693  4734  4734  4846  1402  3192
  1164  5392   679  9385  9963  1130  7827  7191  3653   324  1557  7191
  7329  9218  6974  7634  3412  9901  3873   345  4182  7329 10070  8259
  5259  3653  7191  7329  4303  6119 10115  3925  3897  1317  2706 10448
  6774  1729  9218  9963  3062  4182  4584  4846  7191  6774  4099  7484
  4749  1402  3592  4182  3084  9456   679  8259  4182  7767  8095  5807
  6084  9718  7625  8783  4182  6405 10448  3873  3592  1729  4621  4041
  4846  4182  3372  8475  6774  7625  3653  6405  5259  9609  4621  3036
  6004  1704  4734  4182  3755  7191 10448  5259  3113  3873  5730  8408
  1186  1164  1317  6247  8983  9718  9021  9693  3471  3873  4182  8520
  6162  9963  3592  1262  4734  3592  2361  4013  6405  4041  3653  3372
  8259  9390  4621  5208  6386  3372  6405  1269  4290  8315  4387  3372
  7901   513  9456  3056  2165  7375  3592  3592  7191  4182  7625  1986
  9718  4978  1931  3653  9901  9788  6405  4182  5983  6774  8315  6501
  7539  7191  4800  1722  4749  4182  9390  3873  3275  4182  6183  4099
  7374  7375  9527  3056  7625  6405  5686  8983  4814  5259  7026  8475
 10229  3755  4846  7191  5208  4621  1317   513  4041   282  3592  1722
  1729  6183  8164  6405  6386  7191  5259  9609 10346  7329  1729   705
  2235  1345  4734  1722  3084  3755  5838  8983  1402  6283  3056 10327
  8259  8475  4182  9718]
Mon Dec 17 23:29:45 2018 || train epoch 0 || iter 100 || 4.400512730639819 iters/s || loss 24.531410217285156 || acc 0.0
[[-0.36924246  0.7545861  -1.2369497  ...  3.0858495   0.35915372
  -1.3026253 ]
 [-1.7211065   0.20629707 -2.6521041  ... -0.11544582 -0.16384622
   0.96557724]
 [-1.188537   -1.1743858  -0.5460135  ... -1.8120466  -0.94191694
   2.5358682 ]
 ...
 [ 0.9331041  -1.7122355   1.121377   ... -2.225977    1.6101234
  -0.34366652]
 [ 0.76872224  0.13657439  1.3330904  ... -0.50124514  1.6455736
  -0.84479123]
 [ 0.9072065  -0.37634623 -0.8856757  ...  2.7698164  -0.45090276
  -1.4718833 ]]
[ 7926  7065  6406  3090  5747  7065  6279  3713  6125  7325  5747   819
  6125  7264   706  9260  5888  5702  6499  1625  9100  5702  3885  9859
  9308  7065  3885  4507 10138   819  4455  3090 10142  1843  3824  7465
  5077  7065 10016  8808   216  5697  2000  8325  5697   938  1518  8027
 10028  1019  5702  1621  2893  5109  9141  3824  6759  7065  2805  1136
  9947  7584  6499   819  5888  5702  5702  3627  8321  5888  4835 10138
   706 10558  3754  4835  1136  8564  2000  2786  1318  7127  5888 10419
  5248   990  3885  2405  9977  2005  6719  1910  2806  5117 10572  5702
  1998  9859  5747  5388    41  5697  1527   109  8321  2727  4455  8263
  1026  3978  5117  7513  3713  5089  7926  7467  9947  3771  5702   959
  2000  1790  3615  5289  6499   706  9826  1996  2000  7027  5315  9141
  5888  2805  2554  4814  1144    45  8373  3547  4853  7065  7926  4643
  7405  5702  5702  1998  5053  4309  5702  6604  6793  5264  5702  8015
  7191  6499  2745  5097  6652  9563  6270  6793    41  7926  5544   389
  6654  6759  2554  2786  3062 10250  5802  3615   824  7926  1621  6125
  7926  6499  5289  5747  2727  5702  5927   706  6663  7513  4199  5283
  5702  1886  6652  6499  8930  7103  5562  6577   819  7957  1886 10028
  9348  3587  4495   389  4455  5702  9477  6652  3321    45  3090  5888
  5077  8263    18  6793  3321   382  7465 10003  8072  5913  4283  9977
  5544  7926 10003  4866  2727  3405  1460  6112  4199  9058  3321  1026
  4199  7926  4885   324  9037  2592  9061  7926  2950  7513  9977   824
  5289 10138  2021  5747]
Mon Dec 17 23:30:07 2018 || train epoch 0 || iter 200 || 4.4170776318740685 iters/s || loss 24.317237854003906 || acc 0.0
[[-0.99431074  1.5060194  -2.5387414  ... -1.3770435   1.2629514
   0.8319286 ]
 [ 0.10555135  1.4293411  -2.7048106  ...  1.0069506   0.16005033
   1.8150116 ]
 [ 0.7016947   0.01363127 -1.3668586  ... -1.1660349   3.0597687
   0.86993045]
 ...
 [-3.625283    0.03057871 -2.2419538  ... -1.449473    0.74262464
   0.95621175]
 [-2.8096027   1.3085521  -1.44819    ... -0.61392456 -0.716911
   0.86800694]
 [ 1.1365991   0.61808807  1.2659363  ... -0.12277068  3.422057
   1.1388978 ]]
[ 1460  7026   959  2104  3483  3483  2937  9851  8493   721   131  9576
   872  1082  3405  8470  3673  8054  1313  6719  3403  4637  2104  8228
   872  5142  8178   211  3471  8394  3405  8882  3483  6687   872  1790
   752   737  1654  1082  3710  6367  8178  9220  9026  2104  2104  6719
   721  8145  2104  3236  8394  7107  6840   959 10497  2104  2104  8710
  3463  2104  2104   844  2361  5791  9957  2860  2361  3704  9087  4698
   737  6628  6630  3216  1273  1855  4323  6978  3679  3015 10497   872
  2479  7523  6654  9203  3403  9662  9851  7673  5384  7464  9851 10497
  6687  3405  9786   211  8503  3710  5442   872  8511  8980  5011  9540
  8848   220  6936  9607   679  4105  1162  6632  2361  6719  9588  7789
  9168  6449  8443  8402  3673  4075  3216  4465  8503   959  3216  3483
  6654   872  3028   410  4105 10083  8550  1201   872  5315  1847   737
   959   211  4272  1201  6775  8054  6208  5789   737   520  4075  6719
  6144  7146  7464  2104   721  9851  3216  6632  3699  8443  3643  3471
  5106  3483  8228  3028   737   131  8550  8443  4105   959  1048  8029
   959   872  8443  7464  9407  2104  6654  2104   872  3699  3403  5888
  7191  7483  7040  5096  4982  2034  3471  1468  6654   174   737  8054
  6065  7673  4706  9588   737  3710  7146  8443  9087  6282  8503  4606
  4463  6927  2739  2784  2104  1082  4189  4706   211  4871  9726  4706
   785 10162  8828  3704  1136  8503   959  3679  7483  9660  7146  8792
  1201  8852  8240  6552  8792  2104  5259  6714  7944  6714  8228   559
  3643  8029  8029  8688]
Mon Dec 17 23:30:30 2018 || train epoch 0 || iter 300 || 4.401311915629545 iters/s || loss 23.935443878173828 || acc 0.0
[[ 0.83631766  0.8162152  -1.0920024  ... -0.93904984 -1.1790005
   0.33909082]
 [-2.151044    1.0058001  -2.7907352  ...  0.89805496  0.9213083
  -1.306977  ]
 [-1.054431   -2.0441616  -1.991559   ... -1.3784333  -0.5862409
   0.6319091 ]
 ...
 [-1.2201279  -0.8583488  -4.2909946  ... -2.084325    0.06194796
   3.845408  ]
 [-2.1116047  -3.115324   -5.991016   ...  0.52335286 -0.1133426
  -1.4886225 ]
 [-0.9636546   4.469307    0.5800357  ...  0.5263508  -0.8297082
  -0.63217556]]
[ 2286  3782  6053  1751  7391  8096  9220  3699  1755  3412  7822  8005
  7876  7464  7955  3897  4189  7146  6318  6577   874  9928  8511  7464
  5166  1427  3887  6636  5903   513  2294  3601  5259  3643   621  6238
  8027  5096  7786   900  3631  3403  6157  2079  1755  5732  6409  3888
  7146  2046  7329  9733  8819  8555  5747  8272  6494  8379   328   649
  3885  4953  4014  7325  5794  9598  7511  2079  9833  5794  5560  7561
  4399  1460  8029  3001  7374  3809  1831  4041  8244  3036 10135  3403
  8555  4099  6303  2119  2737  6053  7129   366  4935  6367  1460  9270
  1460  3887  9240  9719  4303  3653  9851  8060  6548  6087 10465  5611
  1118  7855  2187   907  1427  3016  8675 10273  9573  4528  3699  1685
  7405  9957  5702  7822  7955  9733  5907  3631  5816  3887  6190  8677
  4189  6409  5921  2620  2191  3188 10397  1288  4162 10419  9971  4303
  7876  5611  1607  3643  7797  8889  5611  8073  4343  6664  7705  5094
  8868  4637  9971  9253 10534  9733  3699  3479  6548  8029   551  5892
  9091  7561  4402  6849  8859  7876  6775  3243  9719 10419   900  9719
  3631  2746 10534 10534  3471  7378  2079  4057  9971  3831  3001  7023
  9719  9156  9616  8480  9971  2329  8677  9224  9162  3631  7926  4157
   150  6164  4189  2401  9348 10242  1539   328 10419 10559  2079  1037
  5097  9916 10273 10397 10397  8029   150  1831  7108  7876  6053  3631
  5737  6868  8555 10548   959  5903  9662  8658  3687  3872  2479  9091
  6053  6053  6801   150   649  6615   649   328  3412  3237  6428 10419
  2363  7370  5747  2343]
Mon Dec 17 23:30:53 2018 || train epoch 0 || iter 400 || 4.422093323550047 iters/s || loss 23.7889404296875 || acc 0.0
[[ 1.0045254   4.0640597   0.01700912 ...  0.7909425  -0.54761344
  -0.11917926]
 [-2.4195342  -1.8208834  -1.3084354  ... -0.14384614  1.8413819
  -0.2015793 ]
 [ 0.5044471   2.9485366   1.0417839  ... -0.594015    0.71265996
  -0.21228343]
 ...
 [-2.5456927  -2.3372457  -0.8022895  ...  1.2764677  -2.3362224
   0.9675754 ]
 [-2.0049326   0.18761215 -1.0769839  ... -0.61690384  0.09712978
   1.267232  ]
 [-0.89689994  1.3248181  -0.61171675 ...  0.6475204  -1.1069598
  -1.9061003 ]]
[ 6979  5214  1136   674  9617  4606  9379  5534  3239  6202  1356  9015
  2817  8609   352   580  8675  1105  3711  3747  2944  4402  3968  3101
  1544  3104  1965  4935  2858  7325  9585  6755  5461  9889   352  4706
  5477  6053  2135  6701  8882  9238  4706  5534  6327  3219  2491  1041
  7485  8142  4867  4759  2745  7654  4877  7634  4402  2556  7191  6399
  4402  3219  8494  6654  5939  1273  9213  6409  5548  2278  6381  7493
  5653  2556  2718   649  9472  6418  3103  7676 10104   513  4814  2361
  9609  7634  7068  2287  6769  7449  4706  4935  7714  7107  3415  7654
  1790  4980  3430  7511  8934  6769  2678  6260  5534  7736  6043  1345
  8142  4814  6409  5248  1427  6719  8346  8511  2286  7191   352  9777
  5519  2328  9054  3653  2299  4606  6342  4181  9422 10335  4402  9977
  2506  3955  6588  7485  4417  6701  8610  6166  2798  4417  2008  8034
  2506  3699  3372  6131  3745  6343  7817  9998  3745  3071  4081 10283
  2361  3219  2328  9021  2361  6327  2361  7923  8897   445  8134   674
  2556  3415  6755   647  3556  3317  2422 10162  3342  2131  9623  6943
  7513  4402  7182  6367  6032  6367  4105  4706  6701  6588 10162  6260
  6608  6588  1286  6654  1863 10548  2817  2396  9777   901   513  6367
  4689  7197 10007  7023 10162  6666  8710  6367  1790  2792  8373  3711
 10342  4760  7191 10274  6207  7511  6952  3556  3381  6774 10007 10007
  5507  8675  7784  9917  7634  7585   227  4105  4402  1186  3224  2556
  1462  4706   209   775  1507  7585  3704  4417 10335  1710  9195  1916
  6755  1701  6719 10327]
Mon Dec 17 23:31:16 2018 || train epoch 0 || iter 500 || 4.381638729232588 iters/s || loss 23.68936538696289 || acc 0.0
[[-2.489484    0.19623852  1.1852491  ... -1.6472337   1.9068018
  -3.8563907 ]
 [-2.9481287   1.8538132  -4.013986   ... -0.44081804 -1.6220784
   2.0991743 ]
 [ 0.3930647   0.475847   -1.7747536  ... -1.238409    0.25880975
  -0.48226118]
 ...
 [-0.8946403   0.28096083 -3.1935015  ...  0.37694812 -0.6081614
   1.2185088 ]
 [-2.1245165  -1.0953282  -4.977059   ... -0.39528576 -1.8619639
   4.837384  ]
 [-2.3282316  -1.8216994  -3.0806055  ... -0.32929167 -0.81092167
  -0.19554028]]
[ 8560   959  6919  3744  8399 10055  5747 10514  9726  4388  3744  1048
  8407  5094  3976  6249  3097  9267  7646  7470  7068 10086  6040  2000
  4249  1080  4249  6296  8771  7191  8233  1427  2011  3089 10074  4781
  5418  4619   230  2361  6755  4781  9666  3019  4078  3097  6260  4528
  6811   937  1011  6327  9605  6890  8850  3452   288  3746  3725 10155
  8407  3257  6890  4940  9728  9477  6048  3432  9957  9348  6455  1080
   324  7507  3216  6048  8930  8443  2034  4290  4967  6486  6781  5880
  1846  7325  1998  6759 10109  8525  3403  8710  5805  9167  6048  5334
  4781 10335  1792  2008  5697  3898  5236  1773  3126  9403  9091 10427
  4290  5097  7412  5236  8402   453  8798  4290  9777  6048  5418  3403
  8930  5593  5747  6872  1915  4639  6395  5524  6231  6608  6418  3824
  7577  2832  3167  6048  1164  3744  5175  7464  9167  8308  8937  1454
  4248  6759  6048   515  5593  2792  9162  9750  2556  3403  9182  8930
  3699  7495  7075  1998  9348  4649  9750   434  9348  2063  6217  9348
  7374  5697  6425  9132  1986  8346  5961  1416  6421  6048  2328  8447
  8988  3885  6872  7329  9971   178  7165   719  3885  6759  6718 10086
  9202  5236  7513  1080  5236  5175   481  8875  9605  2398  3452  8447
 10217  7325  6053   959  8259  3951  6146  7564  6726  8560  5251   434
 10380  8443  6759 10450  4873  6053  4443   140  7191  4238  6759  6890
  9728  8447  8134  5731  4277  3116  6919  8710  1912 10450   254  9605
  5086  6231  7096  7107  4521  9348  1051  3746  7628  5697  6943  6759
  3167  2063  5697  4537]
Mon Dec 17 23:31:38 2018 || train epoch 0 || iter 600 || 4.408193516809775 iters/s || loss 23.541988372802734 || acc 0.0
[[-2.6375582  -3.1797788  -5.201775   ...  0.2628918   0.32732064
   4.525576  ]
 [-3.3202288  -3.2627263  -3.6262074  ... -1.1184528  -1.5002898
   2.597687  ]
 [-2.9530892   2.526222   -1.1088337  ... -0.9438043   0.09555668
  -0.87232244]
 ...
 [-2.5965376  -2.3898911   1.0444027  ... -0.98095447 -0.6354084
  -0.47637942]
 [-3.2584932   1.3689829  -1.4944142  ...  1.9086932   3.2954187
   2.069919  ]
 [-5.0832067  -3.236545    0.1401501  ...  0.71345    -3.2057958
   1.2461988 ]]
[ 9323  8089  7957  1068  4806  8343  8563  9202 10056  3702  2768  2676
  7951  3685  3863  9647  6183  9370  4290  2384   324  6654  8609  7577
  5291  1893  1749  1893  2700  5490  3951  5842  3885  5074  2036   726
  3412  2137  5353  7908  1150  3167  3403   376  9224  9523  1858  4290
  6442  9202  4189  4034  5697  5074  1893  5876  1622  5802  4619  7372
  9356  8029  8094  5381  3403  5166  7197  5743  8443 10098  5953   324
  4956  5842    23  1080   589  1659  6853  4613  8511  8104  9750  3403
  3400  2935  7547  8448  9985   834  1507  4138   136 10100  3699   294
  6025  3082  2583  9060  3863  2500  2652  9202  4495  6588  5534  4612
  2391  3239  3342   483  6925  2478  8631  7957  9117  8448  9516  9420
  2384   373  5779  3699  9420  8687  5543  5020  9477  6952   589  7146
  5490  2478  1507  5257   589  9885  8010  3907  7204  6829  3625  1507
  6905  4392  8044  1835  4275  5109  5074  4073  3116  5356  8281  8308
  1555  8456  1764  5872  4615  3746  2787   901   901  3679 10013  3403
  1749  9609  5802  5181  6283  7149  8408   671  3863  3796  5236   472
   345  1480  4993  7628   686  7696  6270  2361 10090  7628   873   376
  9726  6183  9026  8624  2486  5897  9609  7585  2635  5897  1507  5026
  9323  8166  6919  9270  3682  9027  1893  4584  9916  6719  8619  1622
  3885  7370  7325  9750  5401  1659  3267  2226  4615  5033   674   895
  8563  9478   324  2287  2361  6395  8408   404  3744  9202  4275  3885
  1216  7374  1544  4290  7023   565  1316  5779  9230  9647  4619  8015
  7202  3713  9270  7539]